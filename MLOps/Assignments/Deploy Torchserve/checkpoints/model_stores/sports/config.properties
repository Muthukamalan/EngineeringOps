default_workers_per_model=1


inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082


# make false or it's taking so much time üëÄ‚è≥ü•±
install_py_dep_per_model=false


load_models=all
max_response_size=655350000
model_store=model_store
workflow_store=model_store
default_response_timeout=600
async_logging=true
disable_token_authorization=true
enable_model_api=true

#### don't increase model copy per worker "overhead". so, system out of error
min_workers=1
max_workers=2


batch_size=10
enable_metrics_api=true
metrics_mode=prometheus
job_queue_size=50

number_of_gpu=0
model_snapshot={\n  "name"\: "startup.cfg",\n  "modelCount"\: 1,\n  "models"\: {\n    "msports"\: {\n      "0.0.1"\: {\n        "defaultVersion"\: true,\n        "marName"\: "msports.mar",\n        "minWorkers"\: 1,\n        "maxWorkers"\: 2,\n        "batchSize"\: 8,\n        "maxBatchDelay"\: 100,\n        "responseTimeout"\: 600,\n        "startupTimeout"\: 120,\n        "runtimeType"\: "python"\n      }\n    }\n  }\n}

# serialize below json and test it
# model_snapshot = {
#     "name":"startup.cfg", 
#     "modelCount":1,
#     "models":{
#         "sd3":{
#             "1.0":{
#                 "defaultVersion":true,
#                 "marName":"sd3.mar",
#                 "minWorkers":2,
#                 "maxWorkers":4,
#                 "batchSize":8,
#                 "maxBatchDelay":10,
#                 "responseTimeout":600
#             }
#         },
#         "sd2":{
#             "1.0":{
#                 "defaultVersion":true,
#                 "marName":"sd3.mar",
#                 "minWorkers":2,
#                 "maxWorkers":4,
#                 "batchSize":8,
#                 "maxBatchDelay":10,
#                 "responseTimeout":600
#             }
#         }

#     }
# }

enable_envvars_config=true
