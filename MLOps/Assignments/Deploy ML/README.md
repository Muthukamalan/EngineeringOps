
<div align="center">
# DogBreedsClassifier

[![python](https://img.shields.io/badge/-Python_3.8_%7C_3.9_%7C_3.10-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)
[![black](https://img.shields.io/badge/Code%20Style-Black-black.svg?labelColor=gray)](https://black.readthedocs.io/en/stable/)
[![isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/) 
![Huggingface](https://img.shields.io/badge/-HuggingFace-FDEE21?style=for-the-badge&logo=HuggingFace&logoColor=black)  <br>
[![codecov](https://codecov.io/gh/ashleve/lightning-hydra-template/branch/main/graph/badge.svg)](https://codecov.io/gh/ashleve/lightning-hydra-template)  <br>
![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)
![DVC](https://img.shields.io/badge/DVC-945DD6?style=for-the-badge&logo=dvc&logoColor=white)
![Amazon S3](https://img.shields.io/badge/Amazon%20S3-FF9900?style=for-the-badge&logo=amazons3&logoColor=white)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white) <br>
![GitHub Actions](https://img.shields.io/badge/github%20actions-%232671E5.svg?style=for-the-badge&logo=githubactions&logoColor=white)
![VS Code Insiders](https://img.shields.io/badge/VS%20Code%20Insiders-35b393.svg?style=for-the-badge&logo=visual-studio-code&logoColor=white)
![Git](https://img.shields.io/badge/git-%23F05033.svg?style=for-the-badge&logo=git&logoColor=white)
![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)
![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=for-the-badge&logo=kaggle&logoColor=white) <br>
[![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/ashleve/lightning-hydra-template#license)


## Main Technologies

[PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) - a lightweight PyTorch wrapper for high-performance AI research. Think of it as a framework for organizing your PyTorch code.

[Hydra](https://github.com/facebookresearch/hydra) - a framework for elegantly configuring complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line.

[DVC](https://dvc.org/) - A tool designed to handle large datasets and machine learning models in a version-controlled workflow

[Tensorboard](https://www.tensorflow.org/tensorboard) - TensorBoard is a tool that provides visualization and debugging capabilities for TensorFlow and PyTorch experiments. Itâ€™s a popular choice for monitoring machine learning training processes in real time.

[AWS|EC2|S3|Lambda|ECR](https://aws.amazon.com/ec2/) - AWS Elastic Compute Cloud (EC2) is a service that provides scalable virtual computing resources in the cloud.

[Docker](https://www.docker.com/) - A platform for creating, deploying, and managing lightweight, portable, and scalable containers.

[Gradio](https://www.gradio.app/) - A Python library for building simple, interactive web interfaces for machine learning models and APIs.



## WORKFLows
[![TrainPipeline](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/ci-train.yml/badge.svg)](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/ci-train.yml)
[![EvalPipeline](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/ci-eval.yml/badge.svg)](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/ci-eval.yml)
[![InferPipeline](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/ci-infer.yml/badge.svg)](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/ci-infer.yml)
[![PYTestPipeline](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/cd-codecov.yml/badge.svg)](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/cd-codecov.yml)
[![GradiDeploy](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/cd-deploy.yml/badge.svg)](https://github.com/Muthukamalan/DogBreedsClassifier/actions/workflows/cd-deploy.yml)

</div>



## Project Structure
```bash
.
â”œâ”€â”€ .devcontainer            <- vscode
â”‚   â””â”€â”€ devcontainer.json
|
â”œâ”€â”€ .github                   <- Github Actions workflows
â”‚   â”œâ”€â”€ ci-eval.yml   
â”‚   â”œâ”€â”€ ci-codecov.yml   
â”‚   â”œâ”€â”€ ci-test.yml
â”‚   â”œâ”€â”€ ci-train.yml
â”‚   â””â”€â”€ ci-deploy.yml
â”œâ”€â”€ assets
â”‚   â”œâ”€â”€ hparams-artifacts.png
â”‚   â”œâ”€â”€ MambaOutHparamSearch.png
â”‚   â”œâ”€â”€ MambaOutHparamsTestScores.png
â”‚   â”œâ”€â”€ OptunaHparams.png
â”‚   â”œâ”€â”€ runner-ec2-training.png
â”‚   â””â”€â”€ self-hosted-runners.png
|
â”œâ”€â”€ configs                                                 <- Hydra configs
â”‚      â”œâ”€â”€ callbacks                                        <- callback config
â”‚      â”‚   â”œâ”€â”€ default.yaml
â”‚      â”‚   â”œâ”€â”€ early_stopping.yaml
â”‚      â”‚   â”œâ”€â”€ learning_rate_monitor.yaml
|      â”‚   â”œâ”€â”€ model_checkpoint.yaml
â”‚      â”‚   â”œâ”€â”€ model_summary.yaml
â”‚      â”‚   â”œâ”€â”€ none.yaml
â”‚      â”‚   â””â”€â”€ rich_progress_bar.yaml
â”‚      â”œâ”€â”€ data                                             <- data config
â”‚      â”‚   â””â”€â”€ dogs.yaml
â”‚      â”œâ”€â”€ debug                                            <- debug config
â”‚      â”‚   â”œâ”€â”€ default.yaml
â”‚      â”‚   â”œâ”€â”€ fdr.yaml
â”‚      â”‚   â”œâ”€â”€ limit.yaml
â”‚      â”‚   â”œâ”€â”€ overfit.yaml
â”‚      â”‚   â””â”€â”€ profiler.yaml
â”‚      â”œâ”€â”€ experiment                                       <- experiment config
â”‚      â”‚   â””â”€â”€ finetune.yaml
â”‚      â”œâ”€â”€ extras                                           <- extras config
â”‚      â”‚   â””â”€â”€ default.yaml
â”‚      â”œâ”€â”€ hparams_search                                   <- hparams config
â”‚      â”‚   â””â”€â”€ mnist_optuna.yaml
â”‚      â”œâ”€â”€ hydra                                            <- hydra config
â”‚      â”‚   â””â”€â”€ default.yaml
â”‚      â”œâ”€â”€ logger                                           <- logger config
â”‚      â”‚   â”œâ”€â”€ aim.yaml
â”‚      â”‚   â”œâ”€â”€ comet.yaml
â”‚      â”‚   â”œâ”€â”€ csv.yaml
â”‚      â”‚   â”œâ”€â”€ default.yaml
â”‚      â”‚   â”œâ”€â”€ many_loggers.yaml
â”‚      â”‚   â”œâ”€â”€ mlflow.yaml
â”‚      â”‚   â”œâ”€â”€ neptune.yaml
â”‚      â”‚   â”œâ”€â”€ tensorboard.yaml
â”‚      â”‚   â””â”€â”€ wandb.yaml
â”‚      â”œâ”€â”€ model                                            <- model config
â”‚      â”‚   â”œâ”€â”€ mamba.yaml
â”‚      â”‚   â”œâ”€â”€ mnist.yaml
â”‚      â”‚   â””â”€â”€ timm_classify.yaml
â”‚      â”œâ”€â”€ paths                                            <- path config
â”‚      â”‚   â””â”€â”€ default.yaml 
â”‚      â”œâ”€â”€ trainer                                          <- trainer config
â”‚      â”‚   â”œâ”€â”€ cpu.yaml
â”‚      â”‚   â”œâ”€â”€ ddp_sim.yaml
â”‚      â”‚   â”œâ”€â”€ ddp.yaml
â”‚      â”‚   â”œâ”€â”€ default.yaml
â”‚      â”‚   â”œâ”€â”€ gpu.yaml
â”‚      â”‚   â””â”€â”€ mps.yaml
â”‚      â”œâ”€â”€ __init__.py
â”‚      â”œâ”€â”€ eval.yaml                                        <- evalution config
â”‚      â””â”€â”€ train.yaml                                       <- training config
|
â”œâ”€â”€ data                                                    <- DATASET
â”‚   â”œâ”€â”€ dogs_dataset
â”‚   â”‚   â”œâ”€â”€ test
â”‚   â”‚   â”œâ”€â”€ train
â”‚   â”‚   â””â”€â”€ validation
â”‚   â””â”€â”€ dogs_dataset.dvc
â”œâ”€â”€ dvc.lock
â”œâ”€â”€ dvc.yaml                                                <- DVC
â”œâ”€â”€ environment.yaml                                        <- conda export `conda env export|grep -v "^prefix: " > environment.yml`
|
|
â”œâ”€â”€ LICENSE
â”œâ”€â”€ logs                   <- Logs generated by hydra and lightning loggers
â”œâ”€â”€ multirun               <- Logs for Hparams Search
â”œâ”€â”€ outputs                <- Logs for eval/fastrun 
|
â”œâ”€â”€ notebooks              <- Jupyter notebooks
|
â”œâ”€â”€ reports
â”‚   â”œâ”€â”€ lr-Adam.png
â”‚   â”œâ”€â”€ test-report.png
â”‚   â”œâ”€â”€ train-report.png
â”‚   â””â”€â”€ val-report.png
|
|
â”œâ”€â”€ samples                        <- inference
â”‚   â”œâ”€â”€ checkpoints 
â”‚   â”‚   â””â”€â”€ epoch_019.ckpt
â”‚   â”œâ”€â”€ inputs
â”‚   â”‚   â”œâ”€â”€ guess1.jpg
â”‚   â”‚   â””â”€â”€ guess2.jpg
â”‚   â””â”€â”€ outputs
|
â”œâ”€â”€ scripts                         <- Shell scripts
â”œâ”€â”€ setup.py                        
|
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ datamodules
â”‚   â”‚   â””â”€â”€ dogs_datamodule.py
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â””â”€â”€ dogs_classifier.py
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ instantiators.py
â”‚   â”‚   â”œâ”€â”€ logging_utils.py
â”‚   â”‚   â”œâ”€â”€ pylogger.py
â”‚   â”‚   â”œâ”€â”€ rich_utils.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ train.py
|   â””â”€â”€ eval.py
|
â”œâ”€â”€ gradio                                  <- GRADIO Space Huggingspace
â”‚   â”œâ”€â”€ .gradio/worflows
â”‚   â”‚    â””â”€â”€ update-space.yaml      
â”‚   â”œâ”€â”€ examples                            <- examples
â”‚   â”‚   â”œâ”€â”€ guess1.jpg
â”‚   â”‚   â””â”€â”€ guess2.jpg
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â”œâ”€â”€ dvc.lock
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ requirements.txt
|
â”œâ”€â”€ tests                                                  <- Pytest
â”‚   â”œâ”€â”€ datamodules
â”‚   â”‚   â””â”€â”€ test_dogs_datamodule.py
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â””â”€â”€ test_dogs_classifier.py
â”‚   â”œâ”€â”€ test_eval.py
â”‚   â””â”€â”€ test_train.py
â”‚
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt                                        <- requirements+GPU
â”œâ”€â”€ requirements.txt.cpu                                    <- requirements+CPU
|
â”œâ”€â”€ Dockerfile                                              <- Dockerfile+GPU
â”œâ”€â”€ Dockerfile.cpu                                          <- Dockerfile+CPU
â”œâ”€â”€ compose.yml                                             <- docker-compose
â”‚   
â”œâ”€â”€ pyproject.toml                                          
â”œâ”€â”€ ruff.toml                                               <- ruff check --fix 
â”œâ”€â”€ pytest.ini                                              <- pytest config
|
â”œâ”€â”€ .env
â”œâ”€â”€ coverage.xml
|
â””â”€â”€ README.md

79 directories, 107 files
```


## Logs

Hydra creates new output directory for every executed run.

Default logging structure:

```log
â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ task_name
â”‚   â”‚   â”œâ”€â”€ runs                        # Logs generated by single runs
â”‚   â”‚   â”‚   â”œâ”€â”€ YYYY-MM-DD_HH-MM-SS       # Datetime of the run
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ .hydra                  # Hydra logs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ csv                     # Csv logs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ wandb                   # Weights&Biases logs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoints             # Training checkpoints
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...                     # Any other thing saved during training
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ multiruns                   # Logs generated by multiruns
â”‚   â”‚       â”œâ”€â”€ YYYY-MM-DD_HH-MM-SS       # Datetime of the multirun
â”‚   â”‚       â”‚   â”œâ”€â”€1                        # Multirun job number
â”‚   â”‚       â”‚   â”œâ”€â”€2
â”‚   â”‚       â”‚   â””â”€â”€ ...
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ debugs                          # Logs generated when debugging config is attached
â”‚       â””â”€â”€ ...
```




## Data Setup
```.env
#AWS
AWS_ACCESS_KEY_ID= 
AWS_SECRET_ACCESS_KEY=
#Dockerhub
DOCKER_USERNAME=
DOCKER_PASSWORD=
#Code-coverage
CODECOV=
#HF
HF_TOKEN=
```
# Runner-Setup
<div align='center'><img src="assets/self-hosted-runners.png" style="width: 100%;"\> </div>


## Clean 
```sh
make trash
make clean
```


## Training

#### fastrun
training simple model
```sh
make fastrun
make sshow
```

### Hparms:: Optuna
<div align='center'><img src="assets/runner-ec2-training.png" style="width: 100%;"\> </div>

##### Loss & Accuracy Curves
- Train DataLoader
- Val DataLoader
<div align='center'><img src="assets/MambaOutHparamSearch.png" style="width: 100%;"\> </div>
- Test DataLoader
<div align='center'><img src="assets/MambaOutHparamsTestScores.png" style="width: 100%;"\> </div>

#### LearningRate
![lr-adam](reports/lr-Adam.png)

### Artifacts in S3 ðŸª£
<div align='center'><img src="assets/hparams-artifacts.png" style="width: 100%;"\> </div>



## Test- PyTest
```sh
make test

============================================================================== test session starts ==============================================================================
platform linux -- Python 3.11.9, pytest-8.3.3, pluggy-1.5.0
rootdir: /home/muthu/GitHub/DogBreedsClassifier
configfile: pytest.ini
plugins: cov-5.0.0, anyio-3.7.1, time-machine-2.15.0, hydra-core-1.3.2
collected 6 items                                                                                                                                                               

tests/datamodules/test_dogs_datamodule.py ...                                                                                                                             [ 50%]
tests/models/test_dogs_classifier.py .                                                                                                                                    [ 66%]
tests/test_eval.py .                                                                                                                                                      [ 83%]
tests/test_train.py .                                                                                                                                                     [100%]
=========================================================================================== warnings summary ============================================================================================
../../miniconda3/envs/venv/lib/python3.11/site-packages/jupyter_client/connect.py:22
  /home/muthu/miniconda3/envs/venv/lib/python3.11/site-packages/jupyter_client/connect.py:22: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs
  given by the platformdirs library.  To remove this warning and
  see the appropriate new directories, set the environment variable
  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.
  The use of platformdirs will be the default in `jupyter_core` v6
    from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write

tests/test_eval.py::test_catdog_ex_testing
tests/test_train.py::test_catdog_ex_training
  /home/muthu/miniconda3/envs/venv/lib/python3.11/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!

tests/test_train.py::test_catdog_ex_training
  /home/muthu/miniconda3/envs/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

tests/test_train.py::test_catdog_ex_training
  /home/muthu/miniconda3/envs/venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================================================================================== warnings summary ============================================================================================
../../miniconda3/envs/venv/lib/python3.11/site-packages/jupyter_client/connect.py:22
  /home/muthu/miniconda3/envs/venv/lib/python3.11/site-packages/jupyter_client/connect.py:22: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs
  given by the platformdirs library.  To remove this warning and
  see the appropriate new directories, set the environment variable
  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.
  The use of platformdirs will be the default in `jupyter_core` v6
    from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write

tests/test_eval.py::test_catdog_ex_testing
tests/test_train.py::test_catdog_ex_training
  /home/muthu/miniconda3/envs/venv/lib/python3.11/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!

tests/test_train.py::test_catdog_ex_training
  /home/muthu/miniconda3/envs/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

tests/test_train.py::test_catdog_ex_training
  /home/muthu/miniconda3/envs/venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================================================================== 6 passed, 5 warnings in 33.11s =====================================================================================
```


## Eval
#### confusion matrix
<!-- - Train DataLoader
<div align='center'><img src="assets/train_confusion_matrix.png" style="width: 100%;"\> </div>

- Val DataLoader
<div align='center'><img src="assets/test_confusion_matrix.png" style="width: 100%;"\> </div>

- Test DataLoader
<div align='center'><img src="assets/val_confusion_matrix.png" style="width: 100%;"\> </div> -->
| Train Matrix    | Val Matrix | Test  Matrix  |
|-----------------|------------|-------------|
| <div align='center'><img src="assets/train_confusion_matrix.png" style="width: 100%;"></div> | <div align='center'><img src="assets/val_confusion_matrix.png" style="width: 100%;"></div> | <div align='center'><img src="assets/test_confusion_matrix.png" style="width: 100%;"></div> |


## prediction
```
args:
    --input_folder
    --output_folder # where to save
    --ckpt_path
```
<div style="display: flex; justify-content: center;">
    <img src="samples/outputs/guess1_prediction.png" style="width: 45%; margin-right: 10px;" />
    <img src="samples/outputs/guess2_prediction.png" style="width: 45%;" />
</div>


## Clean
```sh
make trash
make clean
```


## Inference

```log
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:22 - Starting load_image
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:25 - Finished load_image
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:22 - Starting infer
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:25 - Finished infer
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:22 - Starting save_prediction_image
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:25 - Finished save_prediction_image
<class 'omegaconf.listconfig.ListConfig'> "conv_ratio":         1.2
"depths":             [3, 3, 15, 3]
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:22 - Starting load_image
"dims":               [6, 12, 24, 36]
"head_fn":            default
"in_chans":           3
"lr":                 0.001
"min_lr":             1e-06
"model_name":         Mamba
"num_classes":        10
"pretrained":         False
"scheduler_factor":   0.1
"scheduler_patience": 5
"trainable":          False
"weight_decay":       1e-05
Processed guess2.jpg: Poodle (0.89)
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:25 - Finished load_image
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:22 - Starting infer
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:25 - Finished infer
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:22 - Starting save_prediction_image
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:25 - Finished save_prediction_image
Processed guess1.jpg: Boxer (0.96)
2024-11-10 20:22:17 | INFO     | utils.logging_utils:wrapper:25 - Finished main
```


## Gradio
![Gradio](./assets/gradio-inference.png)